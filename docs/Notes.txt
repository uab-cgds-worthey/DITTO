Getting started
===============

############# 6/5/20 ###################################
srun --ntasks=1 --cpus-per-task=1 --mem-per-cpu=4G --partition=express --job-name=classify --pty /bin/bash
srun --ntasks=1 --cpus-per-task=2 --mem=30G --gres=gpu:4 --partition=pascalnodes --job-name=image_GPU --pty /bin/bash
Using conda envi to install and work on tools
    module load Anaconda3/2020.02
    source activate testing         #for GPU Neural Networks
    conda create --name envi  #first time use only
    source activate envi

ClinVar data (GRCh37) is downloaded to /data/external/ directory using the following command on 6/4/20.
    wget https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh37/clinvar.vcf.gz

To unzip it, use the below command
    gunzip clinvar.vcf.gz

Output CSV file is saved as clinvar.csv under "data/processed"

############# 6/6/20 #########################################
Using Annovar to annotate clinvar variants

Download Annovar - wget http://www.openbioinformatics.org/annovar/download/0wgxR2rIVP/annovar.latest.tar.gz
Unzip Annovar - tar xvfz annovar.latest.tar.gz

Download all annotation databases using the following commands
    perl annotate_variation.pl -buildver hg19 -downdb -webfrom annovar refGene humandb/
    perl annotate_variation.pl -buildver hg19 -downdb cytoBand humandb/
    perl annotate_variation.pl -buildver hg19 -downdb -webfrom annovar exac03 humandb/
    perl annotate_variation.pl -buildver hg19 -downdb -webfrom annovar avsnp147 humandb/
    perl annotate_variation.pl -buildver hg19 -downdb -webfrom annovar dbnsfp30a humandb/

SLURM interactive session before annotating variants
    alias SRUN_EXPRESS="srun --ntasks=1 --cpus-per-task=4 --mem-per-cpu=4096 --partition=express --pty /bin/bash"
    SRUN_EXPRESS

Annotate variants-
    Tab delimited input -
        perl table_annovar.pl ../../processed/clinvar.avinput humandb/ -buildver hg19 -out ../myanno -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation gx,r,f,f,f -nastring . -csvout -polish -xref example/gene_xref.txt
    VCF file input -
        perl table_annovar.pl ../../external/clinvar.vcf humandb/ -buildver hg19 -out ../myanno -remove -protocol refGene,cytoBand,exac03,avsnp147,dbnsfp30a -operation gx,r,f,f,f -nastring . -vcfinput -polish -xref example/gene_xref.txt


Install scikit-allel to extract info from vcf as csv.
    conda install -c conda-forge scikit-allel
Run "parse_clinvar.py" code to convert vcf annotations to txt format.


################## 6/8/20 ###########################################

Ran Brandon's script for annotation
    -Script is located in this path
    /data/project/worthey_lab/projects/experimental_pipelines/annovar_vcf_annotation
    -Commands to Run
        module load Python/3.6.6-foss-2018b
        python /data/project/worthey_lab/projects/experimental_pipelines/annovar_vcf_annotation/Annotate.py --req_AD --clean_homo_ref --clean_no_call /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/external/clinvar_20200602.vcf /data/scratch/tmamidi/ /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/interim/annov/ /data/project/worthey_lab/tools/annovar/annovar_hg19_db


################## 6/24/20 ##########################################

Again, Downloaded the "updated" clinvar vcf file and ran Brandon's script for annotation.

-Commands to Run
        module load Python/3.6.6-foss-2018b
        python /data/project/worthey_lab/projects/experimental_pipelines/annovar_vcf_annotation/Annotate_Sample_Annovar.py  /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/external/clinvar.vcf /data/scratch/tmamidi/ /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/interim/ /data/project/worthey_lab/tools/annovar/annovar_hg19_db

-Ran "parse_clinvar.py" code to convert vcf annotations to txt format.
-Working on "Filter_variants.py" to filter all the variants and their annotations for ML models

################### 7/8/20 ############################################

data/processed/class-count.csv has the summary statistics of all variants categorized according to ACMG guidelines.
Filtering out only the necessary columns -
    Polyphen-2 HDIV vs HVAR - While PolyPhen-2 HDIV uses alleles encoding human proteins and their closely related mammalian homologs as TN observations, PolyPhen-2 HVAR applies common human nsSNVs as TN observations.
    LRTori is a two-sided P-value of the likelihood ratio test of codon constraint. Each LRTori is associated with an estimated nonsynonymous-to-synonymous-rate ratio (Ï‰) and an amino acid alignment of 31 species at the test codon. The score ranges from 0 to 1 and a larger score signifies that the codon is more constrained or a NS is more likely to be deleterious.
    Eigen-PC-raw: Eigen PC score for genome-wide SNVs. A functional prediction score based on conservation, allele frequencies, deleteriousness prediction (for missense SNVs) and epigenomic signals (for synonymous and non-coding SNVs) using an unsupervised learning method (doi: 10.1038/ng.3477).
    GenoCanyon_score: a functional prediction score based on conservation and biochemical annotations using unsupervised statistical learning. (doi:10.1038/srep10576)
	GenoCanyon_rankscore: rank of the GenoCanyon_score among all GenoCanyon_scores in genome

For details on annotations, please refer to this pdf document - https://softgenetics.com/PDF/GeneticistAssistant-Variant-Reference-Fields.pdf
It covers most of the annotations but not all.


### Variants per each ACMG category #########
>>> df[['InterVar_automated','ID']].groupby('InterVar_automated').count()
                            ID
InterVar_automated        Count
Benign                   20859
Likely_benign           187722
Likely_pathogenic        23539
Pathogenic               18587
Uncertain_significance  244031
Total                   494738

>>> df[['CLNSIG','ID']].groupby('CLNSIG').count()

CLNSIG
Affects                                          112
Benign                                         29704
Benign/Likely_benign                           13909
Conflicting_interpretations_of_pathogenicity   20654
Likely_benign                                  80901
Likely_pathogenic                              25774
Pathogenic                                     59416
Pathogenic/Likely_pathogenic                    4072
Uncertain_significance                        193444
association                                      186
drug_response                                    305
not_provided                                    9409
other                                           1720
protective                                        36
risk_factor                                      411
Total	                                        440053

################# 7/16/20 #####################################################################################

There are features annotated multiple times by different databases (using Annovar). Scikit-allel cannot account for multiple features and just omits all other instances except the last one.
To overcome this problem, I'm writing a parsing code for extracting info from vcf - "parse_vcf.py"
Looks like that's not the way to go. It didn't work.

So, the three databases (gnomad211_exome,clinvar_20190305,mitimpact24) while running Annovar are creating duplicate annotations. So, I removed them and ran "Annovar.py" again and it worked.
-Commands to Run
        module load Python/3.6.6-foss-2018b
        python /data/project/worthey_lab/projects/experimental_pipelines/annovar_vcf_annotation/Annovar.py  /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/external/clinvar.vcf /data/scratch/tmamidi/ /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/interim/ /data/project/worthey_lab/tools/annovar/annovar_hg19_db

Then run "parse_clinvar.py" to convert annotated vcf to tab-delimited format.
Finally, got "raw-features.csv" to further transform to categorical and use ML.

CLNSIG	                                        ID
Affects	                                        132
Benign	                                        92165
Benign/Likely_benign	                        19060
Conflicting_interpretations_of_pathogenicity	41019
Likely_benign	                                152762
Likely_pathogenic	                            34029
Pathogenic	                                    74857
Pathogenic/Likely_pathogenic	                6724
Uncertain_significance	                        294609
association	                                    216
association_not_found	                        2
drug_response	                                1979
not_provided	                                11110
other	                                        1660
protective	                                    39
risk_factor	                                    441

################# 7/21/20 ######################################################################

Extract rows with only 5 ACMG classified annotations.
    - (648422, 141)

Impute values to NAs

################# 7/29/20 #######################################################################
Ran the code multiclass.py with SVM classifier for first 10000 rows. Complete dataset takes way too long to output result.

Confusion matrix: Benign. Likely_benign, VUS, Likely_pathogenic, Pathogenic
array([[ 874,  324,    0,    0,   88],
       [  76, 1570,    1,    3,  248],
       [   0,    7,   91,   96,   83],
       [   2,    9,   98,  317,   85],
       [  28,  449,   41,   65, 2945]])

################## 8/1/20 & 8/2/20 ###############################################################
Wrote code for keras using tensorflow (keras-seq) and 5 models from sklearn (models.py).
Using model.job for submitting slurm job.

Keras results - keras.csv, losskeras.jpg and accuracy-keras.jpg - stored in data/processed directory
    5 layers with 141,70,45,23,11,5 neurons and batch size = 100, epoch=10
    looks like the highest is 80% accuracy with almost 60% loss.

sklearn 5 models results - model-scores-<num>.csv
    - for first 1000 lines - repeats=3; split=15
        SVM 0.704 (0.049)
        KNN 0.749 (0.045)
        BAG 0.869 (0.040)
        RF 0.874 (0.042)
        ET 0.869 (0.044)
    -for first 10000 lines - repeats=3; split=50
        SVM 0.748 (0.014)
        KNN 0.709 (0.015)
        BAG 0.806 (0.010)
        RF 0.807 (0.013)
        ET 0.807 (0.012)

################### 8/21/20 #####################################################################
Cleaned the src directory and made subfolders accordingly.
Under classification, developing sklearn pipeline - refer to models.py

################### 8/27/20 ######################################################################
Started working on JSON parsing - output from CodiCem and input for this tool.
Trying out multiprocessing for our models. Ref - https://www.youtube.com/watch?v=fKl2JW_qrso
Parameter tuning -
    https://towardsdatascience.com/credit-risk-management-classification-models-hyperparameter-tuning-d3785edd8371

################### 8/31 & 9/1 ###################################################################
Using Ray, ran multiple classifiers for 5 ACMG classes. results are stored in "sklearn results.xlsx" in prossessed directory.
    code used for this is models.py.
    On a separate context, trying to run commands separately and debug syntax errors in multiclass.py script.

################### 9/2/20 ######################################################################
Start trying out hyperparameter tuning.
    - Trying out Random forest and Decision tree.

################### 9/9/20 ######################################################################
Trying out ELI5 - https://eli5.readthedocs.io/en/latest/overview.html#installation
    - package to calc weights
Better to use feature_importances_ function from classifiers
    - https://machinelearningmastery.com/calculate-feature-importance-with-python/

################### 9/10/20 ######################################################################
Looking more towards Neural Networks which can be easily customizable.
Hyperparameter tuning for NN
    - https://www.youtube.com/watch?v=OSJOBH2-a9Y
    - Also look in to Brandon's code

################### 9/11/20 ######################################################################
Plan for today -
    - Work on creating neural Network
    - use standard scalar for NN (mandatory).
    - Parameter tuning as mentioned by youtube video above and parameters from Brandon's code
    - backpropagation is being taken care by keras. So, no need to implement that separately.
    - select different drop-outs with parameter tuning
    - Use ELI5 package to calculate weights of features once the NN model is trained
    - Use average_precision_score function from sklearn to calculate precision scores.
Also, refer to this for all types of parameters - https://github.com/keras-team/keras/issues/13586
Something to look at for parameter setup - https://towardsdatascience.com/hyperparameter-optimization-with-keras-b82e6364ca53
Breaking down parameters for neural network - https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/
Building layers and neurons - https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_08_4_bayesian_hyperparameter_opt.ipynb

################### 9/14/20 ######################################################################
Converting keras script from CPU to GPU usage on cheaha. This needs Tensorflow-gpu and CUDA to be installed to work.
    - Using the environment 'testing' where I installed tensorflow-gpu.
    - use the below modules on cheaha - also add them to the .job file.
        - module load CUDA/10.1.243
        - module load cuDNN/7.6.2.24-CUDA-10.1.243
Next step is to modify the NN builder definition to create layers and neurons on its own. Use this tutorial for this - https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_08_4_bayesian_hyperparameter_opt.ipynb
Note- I saw a worker stopping and errors out the script. This is cause of njobs=-1 (according to web), where parallel fits take up all the memory and this might exit out? A proposed solution is to update matplotlib (pip install -U matplotlib).
I know it's strange, but whatever works and doesn't throw an error, lol. I have to look up how Cheaha's GPU split jobs among them and how ntasks work.

Cool plots for parameter tuning using hiplot - https://medium.com/roonyx/neural-network-hyper-parameter-tuning-with-keras-tuner-and-hiplot-7637677821fa
Try this for tomorrow. looks promising.
    - https://medium.com/python-data/hyperparameter-tuning-tensorflow-2-models-with-keras-tuner-81f36f801040


################### 9/15/20 ######################################################################
Looks like hyperopt is the way to go. Use this video to build your model - https://www.youtube.com/watch?v=RublDm4J1vY
Probably this helps to copy+paste - https://www.kaggle.com/inspector/keras-hyperopt-example-sketch

################### 9/16/20 ######################################################################
**Ignore everything above about tuning parameters**
BayesSearchCV works using this tutorial - https://github.com/M-e-r-c-u-r-y/Machine-Learning/blob/master/Kaggle/Titanic/Neural%20Network%20Classifier%20using%20Bayesian%20Search.ipynb
    - make sure you have the latest skopt installed.

Now that everything is working, let's move to faster tuning algorithms like Ray.tune - https://docs.ray.io/en/master/tune/examples/index.html
Here is an example of how it's done - https://docs.ray.io/en/master/tune/examples/tune_mnist_keras.html
Didn't understand this, but might be useful, lol - https://medium.com/rapids-ai/30x-faster-hyperparameter-search-with-raytune-and-rapids-403013fbefc5

################### 9/18/20 ######################################################################
Example given by Ray folk - https://colab.research.google.com/github/ray-project/tutorial/blob/master/tune_exercises/exercise_1_basics.ipynb#scrollTo=m1YHm_CyFWS9
Video presentation of the same thing - https://www.youtube.com/watch?v=VX7HvEoMrsA
probably use this - https://medium.com/@himanshurawlani/hyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda

################### 9/19/20 ######################################################################
Build layers and neurons chosing randomly for parameter tuning - https://www.curiousily.com/posts/hackers-guide-to-hyperparameter-tuning/
and this - https://keras-team.github.io/keras-tuner/
but these are available in keras-tuner. Ray doesn't support keras tuner yet.
TuneReporter function definition - https://medium.com/@himanshurawlani/hyperparameter-tuning-with-keras-and-ray-tune-1353e6586fda
Ray tune and this might be a good combination - https://www.justintodata.com/hyperparameter-tuning-with-python-keras-guide/

################### 9/28/20 ######################################################################
As per internet, the way to go is to build the keras model as we already did but use early stopping and population based method to tune parameters (cause it works well with large data and large range of parameters.)
Use this link for Ray tune FAQs - https://github.com/krfricke/ray/blob/3636f77e15f15732ef576d4782705dd1d47b4d6a/doc/source/tune/_tutorials/_faq.rst#id32

################### 9/30/20 ######################################################################
Working with OPTUNA. This allows us to use define-by-run search space. Created a sub-folder under src for optuna runs.

################### 10/1/20 ######################################################################
Optuna is running on cluster. Study name is Ditto_v0
Have to try Axclient - https://www.justintodata.com/hyperparameter-tuning-with-python-keras-guide/
Have to try Ray-tune Dragonfly and Axclient and compare the results. Also use Novorad? amd wandb.

################### 10/2/20 ######################################################################
export CUDA_VISIBLE_DEVICES=4
running the above line gave me a ray hostname to look up for how much I'm using.
Made wrike tasks to use keras-tuner, Ray tune algorithms like hyper-opt and dragonfly along with schedulers.
Also have to tune ML models on the side.

################### 12/16/20 ######################################################################
Making a pipeline of the whole process. Here's the plan -
    -   We need to have a vcf file to start with. I'm taking this one from UDN solves.
        -------------
        52	Proband	UDN308376	C1019-HJ-0012	SL156674	GABRG2	GABRG2-related encephalopathy		GABRG2(NM_00081.3,c.316G>A,p.Ala106Thr,Pathogenic)
        -------------
    -   We have an annotated vcf file for this in our lab workspace on cheaha.
        `python /data/project/worthey_lab/projects/experimental_pipelines/annovar_vcf_annotation/Annovar.py  /data/project/worthey_lab/projects/PyxisMap/analysis/UDN/SL156674.vcf.gz /data/scratch/tmamidi/ /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/interim/ /data/project/worthey_lab/tools/annovar/annovar_hg19_db`
    -   Once we have the annotated vcf file, I'm going to run the following python scripts for annotation and feature extraction. Basically, prepping the data for prediction.
        -   vcf to csv conversion - parse_clinvar.py
        -   csv to test file - filter.py
    -   Now, we'll have the test csv for prediction. We'll be using predict.py for predictions.

For training the classifier/model, I'll be using `parse_clinvar.py` and `filter.py` for prepping the clinvar dataset. Then I'll be running Optuna for hyper-parameter tuning `optuna-tpe.ipy`.
Copy the best parameters and run the final code for model performance stats.

Below are the other cases to work on.
-----------
63	Proband	UDN587784	C1006-HJ-0022	SL201103	ENG	TELANGIECTASIA, HEREDITARY HEMORRHAGIC, TYPE 1; HHT1; SYSTEMIC LUPUS ERYTHEMATOSIS (NO MOLEC DIAGNOSIS)		ENG(NM_000118.3,c.816+6T>C,N/A,Research)
68	Proband	UDN021466	C1012-HJ-0066	SL202061	KMT2B	KMT2B-related dystonia		KMT2B(NM_014727.2,c.7613delC,p.Thr2539Profs*75,Research)		SHANK2(NM_133266.4,c.902dupC,p.Pro302Serfs*32,Pathogenic)
-----------

################### 12/21/20 ######################################################################
*Testing a sample and ranking the variants*
module load Anaconda3/2020.02
source activate envi
cp /data/project/worthey_lab/projects/PyxisMap/analysis/UDN/SL156674.vcf.gz /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/interim/
gunzip /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/interim/SL156674.vcf.gz

We will need to filter the vcf before annotating and filtering as the files are really huge to work with. Using vcflib package to filter.
`conda install -c conda-forge -c bioconda -c defaults vcflib`
https://github.com/vcflib/vcflib - `vcffilter -f "DP > 10 & MQ > 30 & QD > 20" SL156674.vcf > filtered_SL156674.vcf`

python /data/project/worthey_lab/projects/experimental_pipelines/annovar_vcf_annotation/Annovar_Tarun.py  /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/interim/filtered_SL156674.vcf /data/scratch/tmamidi/ /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/interim/ /data/project/worthey_lab/tools/annovar/annovar_hg19_db
KeyError: "['CLNSIG', 'MC', 'CLNVC', 'AF_ESP', 'AF_TGP', 'CLNHGVS', 'CLNREVSTAT', 'AF_EXAC'] not in index"

################### 1/9/21 ######################################################################
Use Annovar.py for Clinvar and Annovar_Tarun.py for samples cause clinvar is not updted and downloaded clinvar has info.
python /data/project/worthey_lab/projects/experimental_pipelines/annovar_vcf_annotation/Annovar.py  /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/external/clinvar.vcf /data/scratch/tmamidi/ /data/project/worthey_lab/projects/experimental_pipelines/tarun/Varsight/data/interim/ /data/project/worthey_lab/tools/annovar/annovar_hg19_db

Imputation notes
Using this for now - logistic regression for transform
    https://medium.com/nerd-for-tech/how-to-implement-mice-algorithm-using-iterative-imputer-to-handle-missing-values-3d6792d4ba7f
Alternate resources -
    https://towardsdatascience.com/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779
    https://www.analyticsvidhya.com/blog/2020/07/knnimputer-a-robust-way-to-impute-missing-values-using-scikit-learn/

Something to read on Overfitting - https://elitedatascience.com/overfitting-in-machine-learning

################### 1/12/21 ######################################################################
Installed Hazel on to my laptop.
Hazel is a gene prioritizing tool given phenotype as input.

################### 1/13/21 ######################################################################
Testing Ditto -
Sample - SL212589	IQCB1	Senior-Loken syndrome5		IQCB1(NM_001023570.3,c.1518_1519delCA,p.His506GlnfsTer13,Pathogenic)	IQCB1(NM_001023570.3,c.1465C>T,p.Arg489Ter,Pathogenic)
HPO - HP:0000510, HP:0003774, HP:0000090
Command for Hazel -
curl -d '{"terms": ["HP:0000479", "HP:0001129", "HP:0003774", "HP:0007481"]}' -H "Content-Type: application/json" \
-X GET http://localhost:5000/v1/cosine > SL212589_genes.yaml

################### 1/18/21 ######################################################################
For faster queries, tabix the file - https://pypi.org/project/pytabix/

################### 1/19/21 ######################################################################
ping Hazel in cheaha - curl -d '{"terms": ["HP:0001337", "HP:0010242"]}' -H "Content-Type: application/json" -X GET http://10.111.161.203:5000/v1/cosine
Looking in to other annotation tools - Trying out Exomiser
Download Exomiser zip from the release page mentioned here and follow the readme in that folder - https://github.com/exomiser/Exomiser
prioritizing variants - http://exomiser.github.io/Exomiser/manual/7/analysis_file_config/

################### 2/10/21 ######################################################################
Started from scratch and using VEP for annotation.
Downloaded new clinvar db and annotated using VEP along with parser that Brandon wrote.
Filtered the data and used it for classification.
Realized that gnomad allele frequencies and CADD scores are available for all variants but dbnsfp have more than 80% missing data.

################### 2/11/21 ######################################################################
Variant annotation on HGMD variants since Clinvar variants have 100% accuracy.
Had to tabix index the HGMD variant file.
`./src/run_pipeline.sh -s -v ../../../../../manual_datasets_central/hgmd/2020q4/hgmd_pro_2020.4_hg38.vcf.gz -o ../data/processed/HGMD -d ~/.ditto_datasets.yaml`

################### 2/15-21/21 ######################################################################
So Tarun has been having trouble annotating HGMD dataset, and after a bit of troubleshooting, it appears this is (highly) likely due to SVs present in them.
Some background: VEP by default can annotate SVs and the max size they allow by default is 10Mb. Also of note is that, VEP by default annotates 5000 variants in parallel. Such parallelization appears to be true for SVs as well.
Taken together, it appears parallelizing SV annotation requires large memory. We tried 100G mem but that was not sufficient either.
I believe we can make the VEP annotation work for SVs by tweaking some settings but my current thinking is that SVs should be out of scope for ditto. At the least, some profiling/analysis on HGMD SVs must be performed before the decision to include them in ditto.
Also, I looked for SVs in clinvar (by searching for SVTYPE tag), and I'm surprised they didn't have any.
PS- I looked at the one of the deletion SVs (size 637Kb), which was successfully annotated. That particular line in VCF had 14578054 characters. Yup, 14.5 million characters

One more error -
So here is the issue that is causing trouble when annotating HGMD vcf with VEP. (To be more specific, bcftools is the one that has trouble). Info field PHEN is not supposed to include semicolon character,

Removing SV from HGMD and working with modified VCF-
Converting VCF of chromosomes '1' to 'chr1' and removing special characters from INFO column -
`
module load Anaconda3/2020.02
module load tabix
module load BCFtools

cp /data/project/worthey_lab/manual_datasets_central/hgmd/2020q4/hgmd_pro_2020.4_hg38.vcf ./`
check all the chromosomes present in the VCF -
`grep -v ^# clinvar.vcf | cut -f1 -d$'\t' | sort -u

sed -E -i 's/(^[^#]+)/chr\1/' <vcf file path>
sed -i 's/^chrMT/chrM/g' clinvar.vcf | grep -v ^chrNW
sed -E 's/(^[^#]+)(=")([^;"]+)(;)+([^;]*?)(")/\1\2\3%3B\5\6/' hgmd_pro_2020.4_hg38.vcf > hgmd_pro_2020.4_hg38_fixed_info.vcf`

or use the VCF that was editied by Brandon
cp /data/project/worthey_lab/projects/experimental_pipelines/brandon_test/hgmd_fix/hgmd_pro_2020.4_hg38_fixed_info.vcf ./
grep -v SVTYPE hgmd_pro_2020.4_hg38_fixed_info.vcf > modified_hgmd.vcf
bcftools norm -f /data/project/worthey_lab/datasets_central/human_reference_genome/processed/GRCh38/no_alt_rel20190408/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna hgmd_pro_2020.4_hg38_edited.vcf -Oz -o hgmd_pro_2020.4_hg38_edited.vcf.gz
#bgzip -c modified_hgmd.vcf > modified_hgmd.vcf.gz
tabix -fp vcf hgmd_pro_2020.4_hg38_edited.vcf.gz`


Run the pipeline for annotation-
`./src/run_pipeline.sh -s -v ../data/external/hgmd_pro_2020.4_hg38_edited.vcf.gz -o ../data/processed/HGMD -d ~/.ditto_datasets.yaml`

Things to note down -
However there is one more thing. Briefly looking at the HGMD dataset, I noticed a deletion SV of size 30 bases. And this makes me wonder what's their min. size definition for a SV. More common limit is 50 bases, but this is opinionated and hence not a surprise they have smaller ones included.
So removing variants based on string SVTYPE would also remove "large" indels. I think this is acceptable but this needs to be kept in mind. Or better yet, log these decisions somewhere.

################### 2/21/21 ######################################################################
Current workflow:
`module load Anaconda3/2020.02
module load tabix
module load BCFtools`
Download Clinvar -
    `wget -P /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/data/external/ https://ftp.ncbi.nlm.nih.gov/pub/clinvar/vcf_GRCh38/clinvar.vcf.gz
    tabix -fp vcf clinvar.vcf.gz `
Check chromosomes and keep note for modifications -
    `zgrep -v ^# clinvar.vcf.gz | cut -f1 -d$'\t' | sort -u`
Copy HGMD VCF -
    `cp /data/project/worthey_lab/manual_datasets_central/hgmd/2020q4/hgmd_pro_2020.4_hg38.vcf ./`
Check chromosomes and keep note for modifications -
    `grep -v ^# hgmd_pro_2020.4_hg38.vcf | cut -f1 -d$'\t' | sort -u`
Fix the INFO column and index for merging -
    `sed -E 's/(^[^#]+)(=")([^;"]+)(;)+([^;]*?)(")/\1\2\3%3B\5\6/' hgmd_pro_2020.4_hg38.vcf > hgmd_pro_2020.4_hg38_fixed_info.vcf`
    bgzip -c  hgmd_pro_2020.4_hg38_fixed_info.vcf > hgmd_pro_2020.4_hg38_fixed_info.vcf.gz
    tabix -fp vcf hgmd_pro_2020.4_hg38_fixed_info.vcf.gz `
Merge Clinvar and HGMD -
    `bcftools merge clinvar.vcf.gz hgmd_pro_2020.4_hg38_fixed_info.vcf.gz -Ov -o ../interim/merged.vcf`
Add `chr` to chromosomes columns -
    `sed -E -i 's/(^[^#]+)/chr\1/' ../interim/merged.vcf `
Fix chromosome issues noted before -
    `sed -i 's/^chrMT/chrM/g' ../interim/merged.vcf
    grep -v ^chrNW ../interim/merged.vcf > ../interim/merged_chr_fix.vcf`
Check chromosomes and fix any remaining issues -
    `grep -v ^# ../interim/merged_chr_fix.vcf | cut -f1 -d$'\t' | sort -u`
Normalize the variants using reference genome -
    `bcftools norm -f /data/project/worthey_lab/datasets_central/human_reference_genome/processed/GRCh38/no_alt_rel20190408/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna ../interim/merged_chr_fix.vcf -Oz -o ../interim/merged_norm.vcf.gz`
Filter variants by size (<30kb) and class -
    `python ../../src/training/data-prep/extract_variants.py`
    Clinvar variants: 300280
    HGMD variants: 156402
bgzip and Tabix index the file -
    `bgzip -c ../interim/merged_sig_norm.vcf > ../interim/merged_sig_norm.vcf.gz
    tabix -fp vcf ../interim/merged_sig_norm.vcf.gz`
Run variant annotation as shown in ReadMe file -
    `./src/run_pipeline.sh -s -v ../data/interim/merged_sig_norm.vcf.gz -o ../data/interim -d ~/.ditto_datasets.yaml`
Parse the annotated vcf file -
    `python parse_annotated_vars.py -i ../data/processed/merged_sig_norm_vep-annotated.vcf.gz -o ../data/processed/merged_sig_norm_vep-annotated.tsv`
Extract Class information for all these variants -
    `python extract_class.py`
Filter, stats and prep the data -
    `python filter.py`

################### 2/22/21 ######################################################################
Working with cross validation and SHAP feature importances. No matter how I filter the data, accuracy remains 99%.


Go ontology, PPI, regulatory network, enzyme activity, binding pockets


################### 2/24/21 ######################################################################
Switching from pandas and scikit-learn to RAPIDS -
conda create -n rapids-0.17 -c rapidsai -c nvidia -c conda-forge \
    -c defaults rapids-blazing=0.17 python=3.7 cudatoolkit=10.1
conda activate rapids-0.17

RAPIDS didn't actually work. So exiting from it.
However, there is a good documentation on it - https://towardsdatascience.com/how-to-use-gpus-for-machine-learning-with-the-new-nvidia-data-science-workstation-64ef37460fa0

One more thing to checkout, "pycaret" - https://github.com/pycaret/pycaret

Voting classifier is working!!!!
`
Model: Ditto_snv
Train_mean_score(train_data)[min, max]: 0.9895633354353022[0.9891826455224171, 0.9897574431754781]
Test_mean_score(train_data)[min, max]: 0.9553676109956326[0.9553676109956326, 0.9553676109956326]
Precision(test_data): 0.9940970722021246
Recall: 0.8649082940969867
roc_auc: 0.9292058306929102
Accuracy: 0.8649082940969867
Confusion_matrix[low_impact, high_impact]:
[[1254381  197263]
 [     51    8898]]
`
`
Model: Ditto_non_snv
Train_mean_score(train_data)[min, max]: 0.9935592590759252[0.9930853864700828, 0.9939860042957113]
Test_mean_score(train_data)[min, max]: 0.9376182240663085[0.9376182240663085, 0.9376182240663085]
Precision(test_data): 0.8891183584829964
Recall: 0.5681611375869461
roc_auc: 0.7449727925202043
Accuracy: 0.5681611375869461
Confusion_matrix[low_impact, high_impact]:
[[27399 27952]
 [   48  9440]]
 `
################### 3/05/21 ######################################################################
Working the past week to tune the classifiers.
Finally, I can make classifiers to tune with Ray Tune PB2. Only downside is that the default parameters perform better than tuned models, lol

################### 3/17/21 ######################################################################
Tuning all the models with TuneSearchCV. They're all under the folder Tuning. Code to tune them -
    `for FILE in Tuning/*.py; do python slurm-launch.py --exp-name $FILE  --command "python $FILE --vtype snv_protein_coding" ; done`

################### 6/27/21 ######################################################################
Working on stabilizing the tuning process the last 2 weeks.
Shifted from `voting classifier` to `stacking classifier` cause it can tune the weightsfor itself. We don't need to manually set them.
Shifted from `tune` parameters to `hyperopt` parametes as they work with conditional search spaces.
Final working script for tuning is at `src/training/training/Tune_hp_stacking.py`.

There was a problem with dbNSFP database used by VEP. VEP doesn't parse out the transcript info from the database but just replaces `;` with `&` and combines all transcripts in to a single line.
We don't want this to happen as we can't work with prediction score from this database for ML. Remember the complex columns that we tried to take mean off? That approach is wrong.
So, Brandon wrote a formatting script to run before running the annotation scripts.
Running the annotation process after formatting the database. Hopefully, it works this time :)

################### 9/3/21 ######################################################################
working on a bunch filters and correlation plots for the last couple weeks.
Also, the ML_models script can plot ROC and AUC curves for each dataset.

Different combinations of filters -
```
python slurm-launch.py --exp-name F_1_0_0_nssnv --command "python training/data-prep/filter.py -v F_1_0_0_nssnv  -af 1 --cutoff 1 -afv 0 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_7_0_0_nssnv --command "python training/data-prep/filter.py -v F_7_0_0_nssnv -af 1 --cutoff 0.7 -afv 0 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_3_0_0_nssnv --command "python training/data-prep/filter.py -v F_3_0_0_nssnv -af 1 --cutoff 0.3 -afv 0 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_1_005_0_nssnv --command "python training/data-prep/filter.py -v F_1_005_0_nssnv -af 1 --cutoff 1 -afv 0.005 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_1_0005_0_nssnv --command "python training/data-prep/filter.py -v F_1_0005_0_nssnv -af 1 --cutoff 1 -afv 0.0005 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_1_00005_0_nssnv --command "python training/data-prep/filter.py -v F_1_00005_0_nssnv -af 1 --cutoff 1 -afv 0.00005 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_7_005_0_nssnv --command "python training/data-prep/filter.py -v F_7_005_0_nssnv -af 1 --cutoff 0.7 -afv 0.005 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_7_0005_0_nssnv --command "python training/data-prep/filter.py -v F_7_0005_0_nssnv -af 1 --cutoff 0.7 -afv 0.0005 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_7_00005_0_nssnv --command "python training/data-prep/filter.py -v F_7_00005_0_nssnv -af 1 --cutoff 0.7 -afv 0.00005 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_3_005_0_nssnv --command "python training/data-prep/filter.py -v F_3_005_0_nssnv -af 1 --cutoff 0.3 -afv 0.005 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_3_0005_0_nssnv --command "python training/data-prep/filter.py -v F_3_0005_0_nssnv -af 1 --cutoff 0.3 -afv 0.0005 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_3_00005_0_nssnv --command "python training/data-prep/filter.py -v F_3_00005_0_nssnv -af 1 --cutoff 0.3 -afv 0.00005 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_1_0_1_nssnv --command "python training/data-prep/filter.py -v F_1_0_1_nssnv -af 1 --cutoff 1 -afv 0 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_7_0_1_nssnv --command "python training/data-prep/filter.py -v F_7_0_1_nssnv -af 1 --cutoff 0.7 -afv 0 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_3_0_1_nssnv --command "python training/data-prep/filter.py -v F_3_0_1_nssnv -af 1 --cutoff 0.3 -afv 0 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_1_005_1_nssnv --command "python training/data-prep/filter.py -v F_1_005_1_nssnv -af 1 --cutoff 1 -afv 0.005 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_1_0005_1_nssnv --command "python training/data-prep/filter.py -v F_1_0005_1_nssnv -af 1 --cutoff 1 -afv 0.0005 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_1_00005_1_nssnv --command "python training/data-prep/filter.py -v F_1_00005_1_nssnv -af 1 --cutoff 1 -afv 0.00005 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_7_005_1_nssnv --command "python training/data-prep/filter.py -v F_7_005_1_nssnv -af 1 --cutoff 0.7 -afv 0.005 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_7_0005_1_nssnv --command "python training/data-prep/filter.py -v F_7_0005_1_nssnv -af 1 --cutoff 0.7 -afv 0.0005 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_7_00005_1_nssnv --command "python training/data-prep/filter.py -v F_7_00005_1_nssnv -af 1 --cutoff 0.7 -afv 0.00005 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_3_005_1_nssnv --command "python training/data-prep/filter.py -v F_3_005_1_nssnv -af 1 --cutoff 0.3 -afv 0.005 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_3_0005_1_nssnv --command "python training/data-prep/filter.py -v F_3_0005_1_nssnv -af 1 --cutoff 0.3 -afv 0.0005 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_3_00005_1_nssnv --command "python training/data-prep/filter.py -v F_3_00005_1_nssnv -af 1 --cutoff 0.3 -afv 0.00005 -otv 1" --mem 30G
python slurm-launch.py --exp-name F_3_no_0_nssnv --command "python training/data-prep/filter.py -v F_3_no_0_nssnv -af 0 --cutoff 0.3 -afv 0.00005 -otv 0" --mem 30G
python slurm-launch.py --exp-name F_3_no_1_nssnv --command "python training/data-prep/filter.py -v F_3_no_1_nssnv -af 0 --cutoff 0.3 -afv 0.00005 -otv 1" --mem 30G
```

Running ML models for different combinations of filters -
```
python slurm-launch.py --exp-name F_1_0_0_nssnv --command "python training/training/ML_models.py -v F_1_0_0_nssnv"
python slurm-launch.py --exp-name F_7_0_0_nssnv --command "python training/training/ML_models.py -v F_7_0_0_nssnv"
python slurm-launch.py --exp-name F_3_0_0_nssnv --command "python training/training/ML_models.py -v F_3_0_0_nssnv"
python slurm-launch.py --exp-name F_1_005_0_nssnv --command "python training/training/ML_models.py -v F_1_005_0_nssnv"
python slurm-launch.py --exp-name F_1_0005_0_nssnv --command "python training/training/ML_models.py -v F_1_0005_0_nssnv"
python slurm-launch.py --exp-name F_1_00005_0_nssnv --command "python training/training/ML_models.py -v F_1_00005_0_nssnv"
python slurm-launch.py --exp-name F_7_005_0_nssnv --command "python training/training/ML_models.py -v F_7_005_0_nssnv"
python slurm-launch.py --exp-name F_7_0005_0_nssnv --command "python training/training/ML_models.py -v F_7_0005_0_nssnv"
python slurm-launch.py --exp-name F_7_00005_0_nssnv --command "python training/training/ML_models.py -v F_7_00005_0_nssnv"
python slurm-launch.py --exp-name F_3_005_0_nssnv --command "python training/training/ML_models.py -v F_3_005_0_nssnv"
python slurm-launch.py --exp-name F_3_0005_0_nssnv --command "python training/training/ML_models.py -v F_3_0005_0_nssnv"
python slurm-launch.py --exp-name F_3_00005_0_nssnv --command "python training/training/ML_models.py -v F_3_00005_0_nssnv"
python slurm-launch.py --exp-name F_1_0_1_nssnv --command "python training/training/ML_models.py -v F_1_0_1_nssnv"
python slurm-launch.py --exp-name F_7_0_1_nssnv --command "python training/training/ML_models.py -v F_7_0_1_nssnv"
python slurm-launch.py --exp-name F_3_0_1_nssnv --command "python training/training/ML_models.py -v F_3_0_1_nssnv"
python slurm-launch.py --exp-name F_1_005_1_nssnv --command "python training/training/ML_models.py -v F_1_005_1_nssnv"
python slurm-launch.py --exp-name F_1_0005_1_nssnv --command "python training/training/ML_models.py -v F_1_0005_1_nssnv"
python slurm-launch.py --exp-name F_1_00005_1_nssnv --command "python training/training/ML_models.py -v F_1_00005_1_nssnv"
python slurm-launch.py --exp-name F_7_005_1_nssnv --command "python training/training/ML_models.py -v F_7_005_1_nssnv"
python slurm-launch.py --exp-name F_7_0005_1_nssnv --command "python training/training/ML_models.py -v F_7_0005_1_nssnv"
python slurm-launch.py --exp-name F_7_00005_1_nssnv --command "python training/training/ML_models.py -v F_7_00005_1_nssnv"
python slurm-launch.py --exp-name F_3_005_1_nssnv --command "python training/training/ML_models.py -v F_3_005_1_nssnv"
python slurm-launch.py --exp-name F_3_0005_1_nssnv --command "python training/training/ML_models.py -v F_3_0005_1_nssnv"
python slurm-launch.py --exp-name F_3_00005_1_nssnv --command "python training/training/ML_models.py -v F_3_00005_1_nssnv"
python slurm-launch.py --exp-name F_3_no_0_nssnv --command "python training/training/ML_models.py -v F_3_no_0_nssnv"
python slurm-launch.py --exp-name F_3_no_1_nssnv --command "python training/training/ML_models.py -v F_3_no_1_nssnv"
```
Tune all classifier -
```
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_7_0_0_nssnv_$FILE --command "python $FILE -v F_7_0_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_1_0_0_nssnv_$FILE --command "python $FILE -v F_1_0_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_1_005_0_nssnv_$FILE --command "python $FILE -v F_1_005_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_1_0005_0_nssnv_$FILE --command "python $FILE -v F_1_0005_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_1_00005_0_nssnv_$FILE --command "python $FILE -v F_1_00005_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_7_005_0_nssnv_$FILE --command "python $FILE -v F_7_005_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_7_0005_0_nssnv_$FILE --command "python $FILE -v F_7_0005_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_7_00005_0_nssnv_$FILE --command "python $FILE -v F_7_00005_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_1_0_1_nssnv_$FILE --command "python $FILE -v F_1_0_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_7_0_1_nssnv_$FILE --command "python $FILE -v F_7_0_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_1_005_1_nssnv_$FILE --command "python $FILE -v F_1_005_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_1_0005_1_nssnv_$FILE --command "python $FILE -v F_1_0005_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_1_00005_1_nssnv_$FILE --command "python $FILE -v F_1_00005_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_7_005_1_nssnv_$FILE --command "python $FILE -v F_7_005_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_7_0005_1_nssnv_$FILE --command "python $FILE -v F_7_0005_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_7_00005_1_nssnv_$FILE --command "python $FILE -v F_7_00005_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_0_0_nssnv_$FILE --command "python $FILE -v F_3_0_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_005_0_nssnv_$FILE --command "python $FILE -v F_3_005_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_0005_0_nssnv_$FILE --command "python $FILE -v F_3_0005_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_00005_0_nssnv_$FILE --command "python $FILE -v F_3_00005_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_0_1_nssnv_$FILE --command "python $FILE -v F_3_0_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_005_1_nssnv_$FILE --command "python $FILE -v F_3_005_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_0005_1_nssnv_$FILE --command "python $FILE -v F_3_0005_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_00005_1_nssnv_$FILE --command "python $FILE -v F_3_00005_1_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_no_0_nssnv_$FILE --command "python $FILE -v F_3_no_0_nssnv" ; done
for FILE in *.py; do python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_no_1_nssnv_$FILE --command "python $FILE -v F_3_no_1_nssnv" ; done
```

Running Stacking Classifier for different combinations of filters -
```
python slurm-launch.py --exp-name F_1_0_0_nssnv_stacking --command "python training/training/stacking.py -v F_1_0_0_nssnv"
python slurm-launch.py --exp-name F_7_0_0_nssnv_stacking --command "python training/training/stacking.py -v F_7_0_0_nssnv"
python slurm-launch.py --exp-name F_3_0_0_nssnv_stacking --command "python training/training/stacking.py -v F_3_0_0_nssnv"
python slurm-launch.py --exp-name F_1_005_0_nssnv_stacking --command "python training/training/stacking.py -v F_1_005_0_nssnv"
python slurm-launch.py --exp-name F_1_0005_0_nssnv_stacking --command "python training/training/stacking.py -v F_1_0005_0_nssnv"
python slurm-launch.py --exp-name F_1_00005_0_nssnv_stacking --command "python training/training/stacking.py -v F_1_00005_0_nssnv"
python slurm-launch.py --exp-name F_7_005_0_nssnv_stacking --command "python training/training/stacking.py -v F_7_005_0_nssnv"
python slurm-launch.py --exp-name F_7_0005_0_nssnv_stacking --command "python training/training/stacking.py -v F_7_0005_0_nssnv"
python slurm-launch.py --exp-name F_7_00005_0_nssnv_stacking --command "python training/training/stacking.py -v F_7_00005_0_nssnv"
python slurm-launch.py --exp-name F_3_005_0_nssnv_stacking --command "python training/training/stacking.py -v F_3_005_0_nssnv"
python slurm-launch.py --exp-name F_3_0005_0_nssnv_stacking --command "python training/training/stacking.py -v F_3_0005_0_nssnv"
python slurm-launch.py --exp-name F_3_00005_0_nssnv_stacking --command "python training/training/stacking.py -v F_3_00005_0_nssnv"
python slurm-launch.py --exp-name F_1_0_1_nssnv_stacking --command "python training/training/stacking.py -v F_1_0_1_nssnv"
python slurm-launch.py --exp-name F_7_0_1_nssnv_stacking --command "python training/training/stacking.py -v F_7_0_1_nssnv"
python slurm-launch.py --exp-name F_3_0_1_nssnv_stacking --command "python training/training/stacking.py -v F_3_0_1_nssnv"
python slurm-launch.py --exp-name F_1_005_1_nssnv_stacking --command "python training/training/stacking.py -v F_1_005_1_nssnv"
python slurm-launch.py --exp-name F_1_0005_1_nssnv_stacking --command "python training/training/stacking.py -v F_1_0005_1_nssnv"
python slurm-launch.py --exp-name F_1_00005_1_nssnv_stacking --command "python training/training/stacking.py -v F_1_00005_1_nssnv"
python slurm-launch.py --exp-name F_7_005_1_nssnv_stacking --command "python training/training/stacking.py -v F_7_005_1_nssnv"
python slurm-launch.py --exp-name F_7_0005_1_nssnv_stacking --command "python training/training/stacking.py -v F_7_0005_1_nssnv"
python slurm-launch.py --exp-name F_7_00005_1_nssnv_stacking --command "python training/training/stacking.py -v F_7_00005_1_nssnv"
python slurm-launch.py --exp-name F_3_005_1_nssnv_stacking --command "python training/training/stacking.py -v F_3_005_1_nssnv"
python slurm-launch.py --exp-name F_3_0005_1_nssnv_stacking --command "python training/training/stacking.py -v F_3_0005_1_nssnv"
python slurm-launch.py --exp-name F_3_00005_1_nssnv_stacking --command "python training/training/stacking.py -v F_3_00005_1_nssnv"
python slurm-launch.py --exp-name F_3_no_0_nssnv_stacking --command "python training/training/stacking.py -v F_3_no_0_nssnv"
python slurm-launch.py --exp-name F_3_no_1_nssnv_stacking --command "python training/training/stacking.py -v F_3_no_1_nssnv"
```

python ../../../slurm-launch.py --num-cpus 20 --mem 250G --partition long -temp ../../../ --exp-name F_3_00005_1_nssnv_stacking --command "python stacking.py -v F_3_00005_1_nssnv"

For testing Ditto -
```
    cp /data/project/worthey_lab/projects/experimental_pipelines/mana/small_tasks/cagi6/rgp/data/processed/annotated_vcf/train/CAGI6_RGP_TRAIN_12_PROBAND_vep-annotated.vcf.gz ./data/processed/testing/
    module load BCFtools
    bcftools annotate  -e'ALT="*" || type!="snp"'  CAGI6_RGP_TRAIN_12_PROBAND_vep-annotated.vcf.gz -Oz -o CAGI6_RGP_TRAIN_12_PROBAND_vep-annotated_filtered.vcf.gz
    python parse_annotated_vars.py -i ../data/processed/testing/CAGI6_RGP_TRAIN_12_PROBAND_vep-annotated_filtered.vcf.gz -o ../data/processed/testing/CAGI6_RGP_TRAIN_12_PROBAND_vep-annotated_filtered.tsv
    python slurm-launch.py --exp-name testing --command "python Ditto/filter.py -i ../data/processed/predictions/CAGI6_RGP_TRAIN_18_PROBAND_vep-annotated_filtered.tsv -O ../data/processed/predictions/CAGI6_RGP_TRAIN_18_PROBAND"
    python slurm-launch.py --exp-name testing --command "python Ditto/predict.py  -i ../data/processed/testing/CAGI6_RGP_TRAIN_12_PROBAND/data.csv -o ../data/processed/testing/CAGI6_RGP_TRAIN_12_PROBAND/predictions.csv -o5 ../data/processed/testing/CAGI6_RGP_TRAIN_12_PROBAND/predictions_500.csv --variant chrX,101412604,C,T"
    python slurm-launch.py --exp-name testing --command "python Ditto/combine_scores.py --sample CAGI6_RGP_TRAIN_12_PROBAND --raw ../data/processed/testing/CAGI6_RGP_TRAIN_12_PROBAND_vep-annotated_filtered.tsv --ditto ../data/processed/testing/CAGI6_RGP_TRAIN_12_PROBAND/ditto_predictions.csv -ep /data/project/worthey_lab/projects/experimental_pipelines/mana/small_tasks/cagi6/rgp/data/processed/exomiser/train/CAGI6_RGP_TRAIN_12_PROBAND -o ../data/processed/testing/CAGI6_RGP_TRAIN_12_PROBAND/predictions_with_exomiser.csv -o100 ../data/processed/testing/CAGI6_RGP_TRAIN_12_PROBAND/predictions_with_exomiser_100.csv"
    python slurm-launch.py --exp-name testing --mem 4G --command "python Ditto/ranks.py -id /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/data/processed/debugged/filter_vcf_by_DP6_AB_hpo_removed --json /data/project/worthey_lab/projects/experimental_pipelines/mana/small_tasks/cagi6/rgp/data/processed/metadata/train_test_metadata_original.json"
    python slurm-launch.py --exp-name testing --command "python Ditto/submission.py"
```




################### 1/31/22 ######################################################################

Download dbSNP database
curl -L https://ftp.ncbi.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/All_20180418.vcf.gz
    data/external/dbsnp_12122_All_20180418.vcf.gz

Creating Ditto db using test vcf data -
```sh
    python src/Ditto/filter.py -i annotation_parsing/.test/testing_variants_hg38_vep-annotated.tsv -O ./data/processed/ditto_db

```

################### 9/21/22 ######################################################################

Running VEP for PKD variants from dbNSFP

```sh
./src/run_pipeline.sh -s -v /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/data/processed/Ditto/dbnsfp_pkd_for_VEP.vcf -o /data/project/worthey_lab/projects/experimental_pipelines/tarun/ditto/data/processed/Ditto/  -d ~/.ditto_datasets.yaml
```
